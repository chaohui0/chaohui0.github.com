---
layout: post
title: Machine Learning 学习笔记(入门）
description: 历时两月，泣血原创
category: blog
---

## Introduce

对于机器学习，森森的觉得，应该花点时间好好学习下基础知识，所以开始认真的学习Ng大神的视频教程。

链接地址：https://class.coursera.org/ml-004/lecture/index

感觉比那个网易公开课的课堂录制视频要简单，还有各种字幕，题目，目标就是一天至少一课吧。

### Supervised Learning

监督学习，即数据集中的每个数据都有相应的正确答案，算法基于这些做出预测

主要有分类问题和回归问题

分类问题就是0-1离散分类

回归问题就是通过回归来预测一个连续值的输出（发现需要补下回归分析，最小二乘法这些大学数学的东西了）

### Unsupervised Learning

无监督学习，与上述的分类问题不同的是，分类问题告诉你很多数据集的结果（比如某种肿瘤大小是否是癌症），而无监督学习的数据集是没有结果的，所以这其实是一个聚类问题,你需要根据数据集获得多个聚类（比如google的新闻聚类，将不同网站的同一条新闻聚合）。

在鸡尾酒会问题上，在多个人同时说话中单独分离出每个人的声音，对，解决这个问题只需要使用一行代码!!介绍了下Octave软件（比matlab还牛逼？）,使用这个建模，然后用c++实现，这样能极大的加快效率（好吧，看来后面还要写一篇octave的入门心得）

（第一课就这样愉快的结束了）


### linear regression

主要讲了cost function，这个其实比较简单，就是误差的平方和除以2m，应该是为了求导的方便吧。

有了cost function，然后讲了下在一个theta参数下是个抛物线，两个theta就是bowl-shape的东东，这样就很自然的引出了等高线(contour figures)的概念。

有了等高线的概念，然后就是介绍传说中的梯度下降（gradient descent)了。

梯度下降中要注意的一点是多参数更新的计算顺序问题，计算每个参数的下一个步长值，要先把所有偏导先算出来，然后计算并赋给新参数，切忌算出一个新参数，再用新参数求另一个参数的偏导。

还有一个新手误区，关于步长的问题

θ:=θ-α·dJ(θ)/dθ
（关于这个公式的由来，个人觉得dJ(θ)/dθ是比较容易理解的，α的引入应该是为了在实际计算中控制迭代速度）

α是learing rate ，随着梯度的不断下降，dJ(θ)/dθ也在不停的变小，所以其实不需要改变α，步长自然就会不断变小。

然后讲了多参数的线性回归的cost funtion 和各个参数的update rule，应该很好理解。

#### 在多参数的线性回归实践中，讲了一些实际使用时的技巧

##### Feature Scaling

当有多个参数的时候，如果多个参数的取值范围差异比较大，则使用梯度下降法的效率比较低（视频中以两个参数为例，当范围相差大，则等高线为长椭圆，下降过程缓慢），所以最好将所有参数采用mean normalization缩放一下,即：

x := （x-样本均值）/样本取值区间长度

（2013-1-15补：练习里一般使用：x := (x-样本均值）/样本标准差）

在选取α的时候，太小会导致下降过程很慢，太大会导致无法convergence

##### Feature的选择和Polynomial Regression

feature的选择很重要，比如选择一个特征：面积 就比选两个特征：长和宽 要好

关于多项式回归，其实应该是feature选择的一个子问题，即将特征的多项式作为一个新的特征，当然要注意参数范围随之相应变化。


##### Normal Equation

用矩阵运算的方式来求解θ，具体过程不赘述

θ = (XTX)-1XTY

这就是我们所说的Normal Equation了。

然后分析了Normal Equation和gradient descent的优缺点。Normal Equation由于是矩阵计算，所以可以不用在意α的选择，而且也不用迭代，但缺点是当特征数量n过大时，由于要计算矩阵的逆，所以计算会很慢。而相对的，gradient descent则在n很大时也能很好的运行。


#### Octave

看了下octave的视频，整体感觉跟matlab差不多，有免费的matlab用也就不折腾octave了，当然对于没好好学过matlab的看一下这几课视频还是很有必要的，对看论文和代码都有帮助。

比如对于vector A 和B 

[A, B] = [A B] 表示横向组合成一个新的vector

[A;B]表示竖向组合成一个vector

对vector A，A'表示A的转置矩阵（transpose）（这个以前论文里看到过，还以为是求导 - -！）


#### Logistic Regression

传说中的逻辑死地回归，主要用于0<=h(x)<=1的分类问题。由于线性分类的不足，比如Tumor size的分类问题，只有当样本空间分布均匀对称的时候有效,而logistic regression很好的解决这类问题。

首先介绍了Logistic function(也叫Sigmoid function，为啥叫这名？）

	g（z）=1/(1+e^(-z))

画一下这个函数曲线就明白了其取值范围为[0,1]。

当然，同样也可以结合多项式回归，使用高次参数来更好的拟合，万变不离其宗，其本质就是：构造关于自变量（可以是多项式）的函数Z，将negative的样本计算的z值小于0，这样g（z）=0，而使positive的样本计算的z值大于0，这样g(z)=1。有了这个模型就可以分类了（理解了好像也很简单）

当然，理论就一句话的事，但实际使用时，怎么去构造这个函数Z呢，这里涉及到两个问题，θ的设置和多项式选择。后者先直接忽略，讲讲θ的选择。

首先还是介绍cost function，形式同线性回归，但是，如果直接套用，会发现这不是一个convex的函数，所以就要变通一下
![logistic_cost_function](/images/logistic_cost_function.png)

一幅图省我千百句啊，总之精妙,所以logistic regression的cost function一定是convex的！

(2013-1-15补：果然没理解就装逼忘得快啊，做题才发现啥都不懂，简单的重新理解下，首先cost function是用来求最小值的，而h(x)的取值范围是0-1，所以当y=1时，h(x)离1越远就让cost function越大，这是合理的，反之y=0亦然）

然后将两个函数合并，怎么合并？由于y只能是0或1，所以好好想想。

	cost (h,y)= ylog(h)+(1-y)log(h)

有了cost function，接下来就是求θ使其最小了。还是使用梯度下降， 

[课程](https://class.coursera.org/ml-004/lecture/36)中第一题几个选项比较能检验之前的内容是否理解，首先A中的cost function是错误的，这是线性回归的cost funtion。

最后两课讲了一些编程实例以及多分类的问题。对n分类问题，采用one-vs-all方法。例如有A，B，C三类，需要对每个class 构造一个hypothesis function（A的hypothesis function的作用是将B和C作为一类，使A和其他两类分开，其他类推），当有个输入x时，对这n个函数求值，将响应值最大的作为输出。

### Overfitting

前面有简略的讲过这个问题，这里详细的讲了一下。首先介绍了一组定义：
underfit （high bias）：表示选择的模型和数据不能很好的拟合（模型过于简单）
overfit（high variance）：当有多个特征的时候，学习到的hypothesis也许能再训练集上获得很好的效果，但不能很好的预测新的数据

然后就是解决overfitting的方法：

（1）减少特征的数量（通常overfitting是由于模型过于复杂，而训练数据过少）

（2）regularization，就是使θ尽量小。具体方式通过在cost function中添加一组regularization term实现，其中λ叫做regularization parameter，注意是从θ1开始。

![regularization](/images/regularization.png)

分别介绍了梯度下降和normal equation的正则化后迭代计算公式。
![gradient_descent_regularization](/images/gradient_descent_regularization.png)

![normal_equation_regularization](/images/normal_equation_regularization.png)

注意，对λ大于0的时候，ng大神说下面那个公式中的XTX一定是可逆的，为神马哪就不知道了。

OK,第三周内容完毕，最后Ng大神说已经学了挺多东西了，凭这些就可以make tonnes of money了，我勒个去，大神你要说话算数啊！


### Neural Networks

看到这一个个传说中的名词，还是挺激动的，且听Ng大神慢慢道来。

首先还是讲了下神经网络算法的应用场景。与线性分类和logistic regression不同的是，这是一种非线性分类方法。

举例一个不规则分类的问题，之前通过构造多项式hypotheses的方式求解，但当feature数量比较多的时候，这就是一个非常麻烦的事情了，而且构造出来的多项式通常还无法很好的拟合真实的分类曲线。

一个具体的问题就是，汽车分类的问题。当有了一堆打了标签的正样本和负样本，然后给机器一张图片，告诉我么这张图片上有没有汽车。首先要选特征（视频中取了两个位置的像素点做特征），然后将正样本的负样本的的特征放在同一个空间中观测，可以发现他们之间是有分界线的（如果没有说明这个特征太稀烂了），然后就可以用我们学到的知识来求得hypothesis函数，并对新的图片分类了。但是，图像特征是很大的，所以线性分类方法对高维特征的处理是比较无力的。忽悠了这么久，开始正式介绍Neural Networks。

首先介绍了Neural Networks的背景，这是一种模拟大脑的算法，而且说了一堆的实验表明只需要一种算法就能实现很多的功能（就像同一块大脑皮质可以响应多个功能）。然后介绍了神经元的工作流程,高中生物的一些知识(后来看了linus的自传，发现这种思想有点类似，我们不应该设计一种只能针对特定问题的方法，而应该是通用的，可扩展的）。

一些名词需要知道。activation，表示一个计算节点，就是一个logistic unit（注意哦，貌似是因为logistic function的判断比较方便）。bias unit，表示一个常量节点。各种layer，input layer，hidden layer， output layer。然后测试题考了个matri of weights controlling funtion mapping from layer j to layer j+1的维度计算，公式是dim（j+1)*(dim(j)+1)，其实就是一个向量乘，θ·X，之所以dim（j）要加1是因为有个bias unit。

然后介绍了为什么Neural Network能获得很复杂的hypothesis，从而解决复杂的分类问题。核心的一点就是每一层都是一个特征选择的过程，然后输出作为下一层的特征输入，而之前介绍的logistic regression作为layer中的一个节点（我怎么想到了单个开关，简单电路），所以一个牛逼的Neural Network就是，很多层的这些单个开关组成的一个复杂电路（好吧高中物理毒害比较深）。这么一想，确实是比较牛逼有木有！

然后举例讲解用logistic function实现逻辑and操作，测试题则是or操作的，突然发现logistic function还是很牛逼的，选它作为activation还是有道理的。接着继续各种not以及同或、亦或等逻辑操作，应该也是毫无鸭梨的。

### Neural Networks :Learning

第五周的课程开始，主要介绍参数的learing，结果第一堂课cost function就看的头大。
![neural_costfunction](/images/neural_costfunction.png)
很长的一串公式，一个一个理解，首先括号内的表示当有K个输出节点的时候，要将这几个输出节点的J(θ)相加,后面的正则化θ其实就是将所有的θ参数列进去（因为θ是个三维矩阵啊），所以就成这样了。

然后介绍求解使得cost function最小的back propagation算法，

首先引入δ变量表示每个节点的“error”，然后，一堆的公式瞬间把我秒杀，硬着头皮看了好几遍，还是没怎么理解，无奈跳过看下个视频，结果尼玛一开头Ng大神说他自己用了这个算法这么多年也不能非常清楚的解释其内涵的理论意义！我勒个去，干嘛不早说！听了两节课，公式就不列了，一个要点是，计算δ是用back propagation，计算z是用forward propagation。然后讲了个实际编程时的unrolling参数的技巧，主要是多参数合并当做一个函数参数来传递的作用吧,主要是两方面：

(1)将高维数组变为一维	A = B(:);     
(2)将一维的切分为多维	C = reshape（A,x,y);     

接着介绍了gradient checking的方法，即用[J(θ+ε)-J(θ-ε)]/2ε计算梯度去验证。

然后讲了random initialization，因为如果不做随机初始化，当所有的θ初始值相同，则会得到相同的激活节点。所以最好将所有θ初始化为[-ε,ε]之间的随机值。

终于，总结下整个算法流程：

（1）选择Network architecture，即网络的层数即每层的activation节点个数,理论上hidden layer要大于等于1层，而且每一层的上的hidden units个数要一样，而且越多越好。

（2）训练：    
		a）随机初始化weights;  
		b）使用forward propagation获得h(x)（即获得样本的输出）;  
		c）计算cost function J(θ);  
		d）使用back propagation计算J(θ)的所有偏导数;  
		e）计算numerical estimate of gradient去做gradient checking;  
		f）使用梯度下降或其他方法使J(θ)最小;  

本周最后一课看了个很有意思的使用了神经网络算法实现自动驾驶的视频。

### Machine Learning Diagnostic

告诉你什么能提高效果，怎么提高算法效果。

首先evaluating a hypothesis的方法的是，将dataset分为训练集和测试集（一般70%做训练），然后用训练集训练参数，如果发生训练集的cost function很小但测试集的很大，那说明overfitting了。

有了上述的判断方法，那怎么选择一个好的模型呢？最简单的方法，将所有的多项式模型都列出来，然后一一计算，按照上面的说法，在测试集上cost function最低就是最好的模型。但是，这可能会存在一个问题，在测试集上跑得好不一定在全集上也跑得好，为了更好的预测算法效果，又引入了交叉验证。即将数据集分为训练集，交叉验证集，测试集三部分，先对根据训练集获得各个模型的参数，然后在交叉验证集上跑一遍，选取最好的模型，再在测试集上跑一下，然后把在测试集上的跑出的precision/recall作为全集的预测效果。

然后是一个high bias 和high variance 的判断，两根error曲线，应该很好理解，训练集和验证集错误率都高的是high bias，一高一低的是high variance。在regularization中，λ的选择也很重要，λ过大会使θ过小，从而产生high bias的模型。λ过小则可能high variance。

learning curve 在high bias的情况下，随着测试集的增大，训练集的error会增大，验证集的error会减少，两者在一定数据量下就会相互靠近并保持几乎水平。要理解其实也很简单，因为high bias的model比较简单，所以较少的数据就能基本确定参数。在high variance的情况下，两个error还是一增一减，但距离较远，需要更多的测试数据才会慢慢靠近。所以在high bias情况下，更多的训练数据是没帮助的，在high variance的情况下，更多的数据还是有帮助的。

然后总结了下各种优化方式适用的情况：

使用更多的训练数据	->	fix high variance  
使用小的特征集		->	fix high variance  
增加features		->	fix high bias  
增加多项式特征		->	fix high bias  
降低 λ			->	fix high bias  
增加 λ			->	fix high variance  

对与神经网络算法，也有overfitting的问题，层数和节点少就说明结构过于简单可能有high bias的问题，层数和节点多则可能有high variance的问题。


### Machine Learning System Design

怎么设计一个机器学习的系统，这是一个很实际的问题。以垃圾邮件判断系统为例，一个概要的过程是，选取一些词作为特征，然后计算邮件的特征，计算并分类。在实际的应用中，可能会遇到拼写错误单词，同义词等处理，比如垃圾邮件为了防止因为出现某些词而被屏蔽，会将这个单词拼写错误。所以，一个牛逼的系统是不停优化的产物。

推荐的流程是：  
（1）从一个能快速实现的简单算法开始，快速验证和测试；  
（2）画出学习曲线，确认什么能提高效果（是否more data， more features等）；  
（3）error analysisi：人工验证bad case，看看是否有系统性错误。比如在垃圾邮件判断系统中，可以检测交叉验证中的错误分类结果，找出主要矛盾（什么情况下最容易分类出错hard negatives），然后找出什么features能提高准确率。

skewed classes:当要分类的类别出现很少，比如Cancer分类中，实际病例概率是0.5%，那使用一个简单的y=0，错误率就只有0.5%(99.5% 的accuracy)。所以，衡量一个算法的好坏只用一个错误率是不科学的，于是引入precision（查准率），recall（召回率，查全率）的概念。

	precision = true positive/(true positive+false positive)  
	recall 	  = true positive/(true positive+false negatives)  

average precision(一般简称AP) 表示两者曲线包含的面积。具体可以参看[链接](http://blog.csdn.net/minnieww/article/details/6991974)

插一句，accuracy和precision虽然是同义词，但区别很大啊  
	accuracy = (true positive+true negatives)/all data  
	all data = (true positive+true negatives+false positive+false negatives)


tradeoff precision and recall. 根据实际需求来调整threshold，调高可以high precision，low recall；And vice versa。怎么根据precision和recall去衡量一个算法的好坏，通常计算方式为：

	score=2*PR(P+R)

本周最后一课，讲了下data，一句名言是：

	It's not who has the best algorithm that wins, It's who has the most data

可见大数据是多么的重要啊。


### Support Vector Machine

终于能听Ng大神讲讲svm了，听完练练手，估计就会牛逼了吧，哈哈。

首先还是介绍svm的理论基础，从logistic regression开始svm的hypothesis function推导，用了两条直线来近似logistic regression的cost function，然后做了下常量变化之类的操作，就得到了:
![svm_hypothesis](/images/svm_hypothesis.png)

svm求解的是最大间距，所以也叫最大间距分类，直观上其实也很好理解。后面从数学上讲解了svm是如何找到最大间距分割线的。上面公式中，当C很大时，前半部需要几乎为0才能使cost function最小，这样，只要求得θ向量长度最小即可。为了准确分类，θTX>=1（或者<=1），当两者方向夹角越小，θ可以用更小的值达到这个目的,而分割线是垂直与θ向量的（因为分割线上是无法分类的，也就是与θ点乘为0的），所以就这样了。

SVM中的kernel应该是最核心也最难理解的东西了。以前看别人的博客，一般都只是一句：kernel函数将低维线性不可分的问题转换到高维从而线性可分。Ng大神首先从一个高斯kernel入手，描述了怎么将一个线性不可分的问题变成线性可分。首先选取了几个landmark点，然后就是构造新的features（将原有的features参照landmark点高斯变换下），然后对于视频中的那个问题就线性可分了。在第二节课中，Ng讲解了如何选择landmark，就是使用训练集中的数据点，可以想象，一个高斯变换加一个正或负的θ参数，就是一个隆起或凹陷，这样就把一个二维的特征向量变成了三维的问题。

对于一些数学的细节可以略过，不过各种参数的作用还是要知道的，其中C=1/λ，所以C越大，会导致high variance，过小会导致high bias。δ过大，则新的features会很平滑，对x的变化反应比较慢，所以会由high bias的问题，反之亦然。



在选择kernel的时候，一个判断的条件是feature的数量n和训练集的数量m，当n很大的时候最好使用logistic regression或者用linear kernel的SVM；当n比较小，m适中的话使用Gaussian kernel的SVM；当n小，m很大的时候，添加更多的features，然后使用logistic regression或者用linear kernel的SVM。

总结一下，SVM通过核函数将特征从低维映射到高维（比如高斯核，其实就增加了一维），然后通过最大间距线性分类，核函数其实也不是那么神秘。

### Unsupervised Learning

无监督学习与有监督学习的显著区别就是，一个没标签，比如聚类操作，而监督学习是由标签的。

聚类算法主要就是k-means了，比较熟悉就不赘述了。要注意的时，k-means一定是cost function下降的（cost function就是每个点到各自中心点的距离之和），但可能不会得到全局最优解，所以可以多次随机初始化，选择一个最优解。还有一个就是K的选择问题，可以通过Elbow method，即画个k-cost 曲线，注意每个k都要多次随机初始化以防止获得一个比较差的局部最优解，随着k的增加，cost一定是下降的（极端情况，k=m则cost=0）。

还有中无监督学习的应用是dimensionality reduction，比如PCA神马的。首先讲了数据压缩和数据可视化，比较简单，降维就是投影到低维。而PCA（principal component analysis）做的就是找到投影距离最短的那个向量，即projection error最小。（课程中盯了那数据张表好久啊，真是少壮不努力一生在内地啊）。看上去PCA和linear regression看上去有点像，不过还是不同的。

在实际应用中，跟有监督学习一样也需要做feature scaling预处理。计算的过程很简单，分两步，分别计算covariance matrix（sigma）和eigenvector，然后选择特征向量的前k个与输入特征相乘就能得到降维后的k维特征了，具体步骤如下图。

![pca](/images/pca.png)

如何选择K也是PCA中一个很重要的问题，通常选择投影误差小于1%的k。实际编程中，使用svd函数返回值的S就可以很容易的实现这个目的。

![pca_choosek](/images/pca_choosek.png)

总体上来说，PCA就是一个高维到低维的线性空间转换，线性代数的东西还的差不多了，理论推导有点晕，但实际应用还是比较简单的。

最后Ng讲了一些PCA使用要注意的地方，比如用PCA来防止overfitting的想法是不对的，因为PCA在降维时只考虑X，而没有考虑lable，所以可能会丢掉一些有用信息。还有Ng建议最好先用原始的features跑一遍，如果实在不行再用PCA。PCA还有一个作用是将高维的参数降维到2-3维来做数据可视化。

### Anomaly Detection

异常检测，简单的说就是根据现有的模型检测一个新来的数据是不是异常的。一个实例就是Fraud detection，比如qq登陆要手动输入验证码就说明你这次登陆被Fraud detection返回true了。

在介绍算法的时候，前提就是每个参数的分布是服从高斯分布的（不禁有个疑问：登陆地点神马的怎么服从正态分布？），然后整体概率就是所有参数的高斯分布乘积，小于一定值就是异常。

在实际应用中，训练数据是大量的正常数据（用正常数据来训练获得各个参数的高斯分布），然后在交叉验证集和测试集中使用带label的（x，y）正常和异常数据。然后是一些要注意的地方，类似skew data中的precision、recall和accuracy的问题。

然后是anomaly detection和supervised learning的对比。首先两者的应用数据量不同，anomaly detection通常只有少量的positive examples（异常数据）和大量的negative examples（正常数据），而supervise learning 通常有大量positive和negative examples。还有更重要的是，异常检测中会有很多种类的异常，在实际检测中可能会出现从没在测试集或交叉验证集中出现的异常数据（其实就是只需要知道正常数据分布模型）。而supervised learning 面对的实际数据是和训练集中类似的（其实就是知道各种分类，包括负样本，的整体分布的模型）。

对于我上面提到的万一features不是服从高斯分布的情况，Ng建议最好将参数变换成类似高斯分布的情况。如果在应用中发现不能很好的检测到异常，应当考虑下增加新的features。

由于上述的方法根据每个参数的高斯分布结果相乘来判断是否异常，对于实际相对各参数非对称的分布是无力的,比如例子中的二维参数斜椭圆分布（其数学意义就是，当参数间无相关性，可以分别独立计算，但当参数间有相关性，就需要合起来计算整体分布），所以引入了Multivariate Gaussian Distribution。其中一个核心的概念就是协方差矩阵（没错，就是PCA中出现的那个covariance matrix），正相关和负相关就由矩阵中的参数决定。

当然，Multivariate Gaussian的计算开销是很大的，所以可以通过构造新的参数来转换为original model解决（比如线性相关的两个参数x1，x2可以构造新的x3=x2/x1）。还有对于Multivariate Gaussian，训练集个数必须大于参数个数，不然协方差矩阵不可逆(Ng建议m>=10n)。

### 推荐系统

推荐系统应该是机器学习中应用最广的一类算法，主要功能就是根据已有的数据计算出未知的数据（比如评分系统中，根据已有的电影评分推断出未看过的电影的评分，从而进行推荐）。首先介绍的的content based recommendation，以电影评分为例，即知道电影的各种特征，根据这些特征和用户评分进行推荐。其过程类似线性分类，也采用低度下降计算各个用户的打分参数。

但是，要只要每个电影的features是很难的，如果知道每个用户的评分标准，那么也可以反过来计算每个电影的features。所以计算用户评分矩阵和电影的features就像是先有鸡还是先有蛋的问题，这样呢就有了collaborative filtering（协同过滤）的概念。
![collaborative_filtering](/images/collaborative_filtering.png)
然后，使用梯度下降计算参数
![collaborabive_gradient_descent](/images/collaborabive_gradient_descent.png)

### 大数据机器学习

机器学习的数据量应该是越大越好，但当训练集很大，使用传统的梯度下降法（batch gradient descent）将非常低效。因此引入了一种随机梯度下降方法（stochastic gradient descent）。其主要步骤是：  
1）随机排序训练数据；  
2）对每个训练数据依次计算θ=θ-α(h(x)-y)x，重复几遍就ok

还有一些变形方案，比如mini-batch gradient descent，就是每一步的梯度计算有较少的训练集（比如10个），具体计算过程同上，这个速度应该比上述两种都快。

查看算法是否convergence，还是要画线，根据迭代次数查看error是否在下降。在随机梯度下降中，通常最后不会收敛到局部最佳位置，可以通过慢慢减小步长α来缓解这个问题。

然后介绍了online learning，即根据用户点击等信息进行学习推荐。

当数据量很大的时候，Ng更推荐的方式是使用MapReduce等并行计算方式解决。

### Photo OCR

Ng以Photo OCR为例讲解了一个机器学习算法的设计过程，首先分解成一个工作pipeline，然后构造训练数据！这点非常重要，Ng讲了几次其实构造十倍的训练数据不需要很大的代价，但是其效果是惊人的。具体到OCR，跟行人检测一样，也采用滑动窗口的方式，具体看DMP算法。然后介绍了如何判断应该提高pipeline中的哪个part的ceiling analysis方法，具体方式就是将每一步用手动保证100%准确率，看整体准确率的提高，如果提高不大那就不要浪费时间在提高该部分的算法了,机器学习还是不够智能啊。全剧终，符总结截图  
![summation](/images/summation.png)

have fun! @2014-1-7 19:11

### 习题及答案

终于把练习题全部做完了，大家可以拿去参考一下，请点击查看[git地址](https://github.com/chaohui0/MLExcercises)

本篇暂时完结 @2014-2-11 10:38




[LinChaohui]:    http://www.linchaohui.com  "LinChaohui"
