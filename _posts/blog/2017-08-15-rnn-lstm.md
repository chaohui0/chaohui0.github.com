---
layout: post
title: RNN与LSTM与tensorflow
description: 深度学习
category: blog
---

看了下第一次写深度学习的文章还是三年多以前的事了，当时用的还是caffe，当时深度学习还没那么火，当时我还不知道到底要干嘛。三年过去了，现在还在写RNN和LSTM的科普文章，真的成绩不大啊。

一般现在说到RNN都是用的LSTM了，用tensorflow训练完模型后，其实就保存了两个变量：lstm/rnn/basic_lstm_cell/weights，lstm/rnn/basic_lstm_cell/biases。这两个变量的维度怎么计算的呢，搜了很久也没看到有人说，其实理解了这个基本上所有过程也就理解了。RNN主要有几个参数：input_size，num_step,h_hidden,output_size。input_size就是输入特征的维度，num_step就是RNN的自循环次数，h_hidden是隐藏层维度，output_size是最终输出结果维度。weights的维度 = (input_size + h_hidden) * h_hidden * 4，因为有4个门，所以要乘以4，又因为RNN的自循环是要考虑输入和之前一个状态的，所以要两者相加，biases的维度 = h_hidden * 4。


插个小插曲，在导出tensorflow模型的时候也是发现了一些细节的坑，而之前狗屎运没注意到的，然后这次就坑到了。保存模型的时候用了下面两行代码，这样可以保存所有的变量，但是，你必须要注意saver定义的位置必须在所有训练变量定义完之后。

	saver = tf.train.Saver()
	path = saver.save(sess, FLAGS.checkpoint_file)

这次就是把第一行代码放到变量定义之前了，造成了模型没保存出来！为什么呢，因为这里的Saver初始化其实是有输入参数的，默认是所有的变量，如果你在变量定义前生成了saver对象，那里面就是空的！一般可以等训练完之后，再生成saver对象并保存。

而在加载模型的时候，一段完整示例如下，需要注意的是定义前向计算socre的时候，reuse是False，也就是说要重新定义那些训练变量，之后再生成saver兑现并调用saver.restore来加载保存在模型中的那些训练变量值。这样tensorflow的模型保存和加载就都没问题了！！
	
	model = TreasureModel()
	test_fea = tf.placeholder(tf.float32, shape=(1,input_size ))
	y_pred = model.scores(test_fea,1.0,False,None)
	init = tf.initialize_all_variables()
	with tf.Session() as sess:
		sess.run(init)
		saver = tf.train.Saver()
		saver.restore(sess, FLAGS.checkpoint_file)
		t_vars = tf.trainable_variables()
		coord = tf.train.Coordinator()
		threads = tf.train.start_queue_runners(sess=sess, coord=coord)
		y = sess.run(y_pred, {test_fea:[[1.0,2.0,3.0,4.0]]})
		coord.request_stop()
		coord.join(threads)
	
	
想到哪说到哪，关于tanh激活函数，其实就是2*(sigmoid(2x)) - 1 就是把sigmoid的放大平移，使其均值为0，范围为[-1,1]

应该要把tensorflow导出的模型用c++重写前向过程，所以还需要知道4个门的顺序。

BasicLSTMCell的源码在python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py，可以通过修改__call__方法来了解整个前向计算的过程，在这放点干货吧，哥把整个前向过程重写了，跟tensorflow的BasicLSTMCell输出是完全一样的，以下是一个完整的示例代码，有没有发现，其实没几行代码，但哥搞了好几天，主要是各种矩阵操作绕的头都大了！

	import os,sys
	reload(sys)
	sys.setdefaultencoding('utf8')

	import numpy as np
	import pandas as pd

	input_size = 4 
	output_size = 2
	dim_vec = 2
	frame_num = 2
	hidden_size = 2


	def sigmoid(z):
		s = 1.0 / (1.0 + np.exp(-1.0 * z))
		return s

	def tanh(z):
		return 2*sigmoid(2*z) - 1

	weights = [0.0274598,-0.173511,0.419493,-0.465369,0.677011,0.0482636,0.509747,-0.638336,-0.316329,-0.356314,0.294494,0.151184,-0.398772,-0.374825,0.177627,0.044093,0.424177,0.592243,0.346691,-0.558091,0.555613,-0.0171373,0.552993,-0.662974,-0.354132,-0.251043,0.238877,0.559065,0.394596,-0.00792031,-0.178783,-0.384413]
	weights = np.reshape(weights,[4*hidden_size,hidden_size+dim_vec])
	bias = [0.000999977,0.000999979,-0.000999945,-0.000999978,0.000996455,0.000999826,0.000999989,0.000999994]

	last_state = [0,0]
	last_c = [0,0]
	input_list = [[29,22],[20,10]]

	for p_in in input_list:
		p_in = np.concatenate((p_in,last_state))
		p_in = np.reshape(p_in,[-1,input_size])
		concat = np.matmul(p_in,np.transpose(weights))

		# i = input_gate, j = new_input, f = forget_gate, o = output_gate
		i, j, f, o = np.split(concat[0],4)
		new_c = (last_c * sigmoid(f) + sigmoid(i) * tanh(j))
		new_h = tanh(new_c) * sigmoid(o)

		print new_c,new_h
		last_state = new_h
		last_c = new_c


了解tensorflow的模型数据结构和整个LSTM的运算过程，就可以用c++重写了。

意外收获一个c++的debug调试神器，示例命令如下

cgdb --args ./nnet-forward model.txt ark,t:sample_input.txt ark,t:sample_out.txt



[LinChaohui]:    http://www.linchaohui.cn  "LinChaohui"
