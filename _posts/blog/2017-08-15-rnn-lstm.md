---
layout: post
title: RNN与LSTM
description: 深度学习
category: blog
---

看了下第一次写深度学习的文章还是三年多以前的事了，当时用的还是caffe，当时深度学习还没那么火，当时我还不知道到底要干嘛。三年过去了，现在还在写RNN和LSTM的科普文章，真的成绩不大啊。

一般现在说到RNN都是用的LSTM了，用tensorflow训练完模型后，其实就保存了两个变量：lstm/rnn/basic_lstm_cell/weights，lstm/rnn/basic_lstm_cell/biases。这两个变量的维度怎么计算的呢，搜了很久也没看到有人说，其实理解了这个基本上所有过程也就理解了。RNN主要有几个参数：input_size，num_step,h_hidden,output_size。input_size就是输入特征的维度，num_step就是RNN的自循环次数，h_hidden是隐藏层维度，output_size是最终输出结果维度。weights的维度 = (input_size + h_hidden) * h_hidden * 4，因为有4个门，所以要乘以4，又因为RNN的自循环是要考虑输入和之前一个状态的，所以要两者相加，biases的维度 = h_hidden * 4。

应该要把tensorflow导出的模型用c++重写前向过程，所以还需要知道4个门的顺序。





[LinChaohui]:    http://www.linchaohui.cn  "LinChaohui"
