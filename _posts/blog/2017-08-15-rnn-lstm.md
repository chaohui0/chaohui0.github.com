---
layout: post
title: RNN与LSTM与tensorflow
description: 深度学习
category: blog
---

看了下第一次写深度学习的文章还是三年多以前的事了，当时用的还是caffe，当时深度学习还没那么火，当时我还不知道到底要干嘛。三年过去了，现在还在写RNN和LSTM的科普文章，真的成绩不大啊。

一般现在说到RNN都是用的LSTM了，用tensorflow训练完模型后，其实就保存了两个变量：lstm/rnn/basic_lstm_cell/weights，lstm/rnn/basic_lstm_cell/biases。这两个变量的维度怎么计算的呢，搜了很久也没看到有人说，其实理解了这个基本上所有过程也就理解了。RNN主要有几个参数：input_size，num_step,h_hidden,output_size。input_size就是输入特征的维度，num_step就是RNN的自循环次数，h_hidden是隐藏层维度，output_size是最终输出结果维度。weights的维度 = (input_size + h_hidden) * h_hidden * 4，因为有4个门，所以要乘以4，又因为RNN的自循环是要考虑输入和之前一个状态的，所以要两者相加，biases的维度 = h_hidden * 4。

应该要把tensorflow导出的模型用c++重写前向过程，所以还需要知道4个门的顺序。

插个小插曲，在导出tensorflow模型的时候也是发现了一些细节的坑，而之前狗屎运没注意到的，然后这次就坑到了。保存模型的时候用了下面两行代码，这样可以保存所有的变量，但是，你必须要注意saver定义的位置必须在所有训练变量定义完之后。

	saver = tf.train.Saver()
	path = saver.save(sess, FLAGS.checkpoint_file)

这次就是把第一行代码放到变量定义之前了，造成了模型没保存出来！为什么呢，因为这里的Saver初始化其实是有输入参数的，默认是所有的变量，如果你在变量定义前生成了saver对象，那里面就是空的！一般可以等训练完之后，再生成saver对象并保存。

而在加载模型的时候，一段完整示例如下，需要注意的是定义前向计算socre的时候，reuse是False，也就是说要重新定义那些训练变量，之后再生成saver兑现并调用saver.restore来加载保存在模型中的那些训练变量值。这样tensorflow的模型保存和加载就都没问题了！！
	
	model = TreasureModel()
	test_fea = tf.placeholder(tf.float32, shape=(1,input_size ))
	y_pred = model.scores(test_fea,1.0,False,None)
	init = tf.initialize_all_variables()
	with tf.Session() as sess:
		sess.run(init)
		saver = tf.train.Saver()
		saver.restore(sess, FLAGS.checkpoint_file)
		t_vars = tf.trainable_variables()
		coord = tf.train.Coordinator()
		threads = tf.train.start_queue_runners(sess=sess, coord=coord)
		y = sess.run(y_pred, {test_fea:[[1.0,2.0,3.0,4.0]]})
		coord.request_stop()
		coord.join(threads)
	
	



[LinChaohui]:    http://www.linchaohui.cn  "LinChaohui"
